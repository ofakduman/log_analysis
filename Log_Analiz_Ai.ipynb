{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aigdeli\\AppData\\Local\\anaconda3\\envs\\log_analiz_ai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\aigdeli\\AppData\\Local\\anaconda3\\envs\\log_analiz_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-25 10:37:31.776 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\aigdeli\\AppData\\Local\\anaconda3\\envs\\log_analiz_ai\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.chart import BarChart, Reference\n",
    "from openpyxl.drawing.image import Image\n",
    "from fpdf import FPDF, XPos, YPos\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Loglama ayarları\n",
    "logging.basicConfig(filename='error_log_processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# HuggingFaceEndpoint modelini başlatma\n",
    "model = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    temperature=0.7, \n",
    "    max_length=100,\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Error zaman dilimlerindeki sayıları hesaplayan fonksiyon\n",
    "def count_errors_in_time_intervals(error_times):\n",
    "    error_counts = {}\n",
    "    for time in error_times:\n",
    "        hour_minute_second = time.strftime('%H:%M:%S')\n",
    "        if hour_minute_second not in error_counts:\n",
    "            error_counts[hour_minute_second] = 1\n",
    "        else:\n",
    "            error_counts[hour_minute_second] += 1\n",
    "    return error_counts\n",
    "\n",
    "# Zaman dilimlerine göre en sık rastlanan hataları hesaplayan fonksiyon\n",
    "def most_frequent_errors_by_time(error_times, error_descriptions):\n",
    "    error_time_description = {}\n",
    "    for time, description in zip(error_times, error_descriptions):\n",
    "        hour_minute_second = time.strftime('%H:%M:%S')\n",
    "        if description not in error_time_description:\n",
    "            error_time_description[description] = {hour_minute_second: 1}\n",
    "        elif hour_minute_second not in error_time_description[description]:\n",
    "            error_time_description[description][hour_minute_second] += 1\n",
    "        else:\n",
    "            error_time_description[description][hour_minute_second] += 1\n",
    "    return error_time_description\n",
    "\n",
    "\n",
    "# Belirli bir yapıdaki cümleleri sayan fonksiyon\n",
    "def count_specific_structure_sentences(log_list):\n",
    "    cleaned_logs = clean_logs(log_list)\n",
    "    structure_counter = Counter()\n",
    "    structure_pattern = re.compile(r'\\b[A-Za-z\\s]+\\b')\n",
    "    for log in cleaned_logs:\n",
    "        structures = structure_pattern.findall(log)\n",
    "        specific_structure = [structure for structure in structures if 5 <= len(structure.split()) <= 20]\n",
    "        structure_counter.update(specific_structure)\n",
    "    return structure_counter\n",
    "# Log dosyalarını temizleyen fonksiyon\n",
    "def clean_logs(log_list):\n",
    "    cleaned_logs = []\n",
    "    for log in log_list:\n",
    "        log = re.sub(r'\\[WEBGATE\\]', '', log)\n",
    "        log = re.sub(r'at [\\s\\S]*', '', log)\n",
    "        log = re.sub(r'SessionID:.*', '', log)\n",
    "        log = re.sub(r'\\{0\\}', '', log)\n",
    "        log = re.sub(r'\\[.*?\\]', '', log)\n",
    "        log = re.sub(r'Risk durumu: \\d+:', '', log)\n",
    "        log = re.sub(r'Satılabilir stok : \\d+,\\d+', '', log)\n",
    "        log = re.sub(r'Portfoyno: \\d+', '', log)\n",
    "        log = re.sub(r'\\[TimesTen\\]\\[\\w+\\s\\d+\\.\\d+\\.\\d+\\.\\d+ ODBC Driver\\]\\[TimesTen\\]', '', log)\n",
    "        log = re.sub(r'java.sql.SQLException: .*', '', log)\n",
    "        log = re.sub(r'EYSException :.*', '', log)\n",
    "        log = re.sub(r'at .* \\(.*\\)\\s*', '', log)\n",
    "        log = re.sub(r'Kullanici kodu: \\w+', '', log)\n",
    "        log = re.sub(r'EmirNo: \\d+', '', log)\n",
    "        \n",
    "        cleaned_logs.append(log.strip())\n",
    "    return cleaned_logs\n",
    "\n",
    "# Loglardan hata mesajlarını çıkaran fonksiyon\n",
    "def extract_errors_from_log(log):\n",
    "    # Hata mesajlarını tespit etmek için regex deseni\n",
    "    error_pattern = re.compile(r'ERROR:.*|Exception:.*', re.IGNORECASE)\n",
    "    errors = error_pattern.findall(log)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def generate_analysis_pdf(analyses):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Helvetica\", size=12)\n",
    "\n",
    "    pdf.cell(200, 10, text=\"Excel Analysis Report\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='C')\n",
    "\n",
    "    for sheet_name, analysis in analyses.items():\n",
    "        pdf.cell(200, 10, text=f\"Sheet: {sheet_name}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "        pdf.multi_cell(200, 10, text=analysis)\n",
    "\n",
    "    return bytes(pdf.output())\n",
    "\n",
    "\n",
    "def analyze_excel(file_path):\n",
    "    # Excel dosyasını oku\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    # Her sayfayı incele ve modelden analiz iste\n",
    "    analyses = {}\n",
    "    for sheet_name, data in df.items():\n",
    "        summary = data.describe(include='all')\n",
    "        print(len(summary))\n",
    "        analysis_text = model.invoke(f\"Summarize the following data analysis for the sheet '{sheet_name}':\\n{summary}\")\n",
    "        analyses[sheet_name] = analysis_text\n",
    "\n",
    "    return analyses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hatalara yönelik çözüm önerileri üretme fonksiyonu\n",
    "# Hatalara yönelik çözüm önerileri üretme fonksiyonu\n",
    "def generate_solutions(cleaned_logs):\n",
    "    solutions = {}\n",
    "    for log in cleaned_logs:\n",
    "        errors = extract_errors_from_log(log)  # Logdan hataları çıkar\n",
    "        for error in errors:\n",
    "            # Hataları küçük parçalara ayırarak gönder\n",
    "            split_errors = [error[i:i+100] for i in range(0, len(error), 100)]\n",
    "            for split_error in split_errors:\n",
    "                result = model.invoke(split_error)  # Modelin çözüm önerisi üretmesi\n",
    "                solutions[split_error] = result\n",
    "    return solutions\n",
    "\n",
    "\n",
    "\n",
    "# PDF raporu oluşturma fonksiyonu\n",
    "def generate_pdf_report(structure_sentences, solutions, categorized_errors):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Helvetica\", size=12)  # Arial yerine Helvetica\n",
    "\n",
    "    pdf.cell(200, 10, text=\"Error Log Analysis Report\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='C')\n",
    "    \n",
    "    pdf.cell(200, 10, text=\"Specific Structure Sentences:\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    for sentence, count in structure_sentences.items():\n",
    "        pdf.cell(200, 10, text=f\"{sentence}: {count}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    \n",
    "    pdf.cell(200, 10, text=\"Solutions:\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    for error, solution in solutions.items():\n",
    "        pdf.cell(200, 10, text=f\"{error}: {solution}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "\n",
    "    pdf.cell(200, 10, text=\"Error Categories:\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    for category, count in categorized_errors.items():\n",
    "        pdf.cell(200, 10, text=f\"{category}: {count}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    \n",
    "    return bytes(pdf.output())  # PDF'yi bytes olarak döndür\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Excel dosyasını oluşturma fonksiyonu\n",
    "def generate_excel_report(logs, error_times, error_descriptions):\n",
    "    output = io.BytesIO()\n",
    "    workbook = Workbook()\n",
    "\n",
    "    # Sayfa 1: Error Logs\n",
    "    sheet1 = workbook.active\n",
    "    sheet1.title = \"Error Logs\"\n",
    "\n",
    "    # Başlıklar\n",
    "    sheet1.append([\"Tarih-Saat\", \"Hata Açıklaması\"])\n",
    "\n",
    "    for log in logs:\n",
    "        if isinstance(log, dict):\n",
    "            sheet1.append([log['Tarih-saat'], log['Error açıklaması']])\n",
    "        else:\n",
    "            # Handle log if it's a string, perhaps split by a delimiter\n",
    "            log_parts = log.split(' ')  # Adjust the delimiter as needed\n",
    "            sheet1.append([log_parts[0], ' '.join(log_parts[1:])])\n",
    "\n",
    "    # Sayfa 2: Error Counts by Time\n",
    "    sheet2 = workbook.create_sheet(title=\"Error Counts by Time\")\n",
    "    sheet2.append([\"Tarih-Saat\", \"Hata Sayısı\"])\n",
    "\n",
    "    error_counts = count_errors_in_time_intervals(error_times)\n",
    "    for time, count in error_counts.items():\n",
    "        sheet2.append([time, count])\n",
    "\n",
    "    # Sayfa 3: Frequent Errors by Time\n",
    "    sheet3 = workbook.create_sheet(title=\"Frequent Errors by Time\")\n",
    "    sheet3.append([\"Hata Açıklaması\", \"Tarih-Saat\", \"Hata Sayısı\"])\n",
    "\n",
    "    frequent_errors = most_frequent_errors_by_time(error_times, error_descriptions)\n",
    "    for description, times in frequent_errors.items():\n",
    "        for time, count in times.items():\n",
    "            sheet3.append([description, time, count])\n",
    "\n",
    "    workbook.save(output)\n",
    "    output.seek(0)\n",
    "    return output\n",
    "\n",
    "# Hataları kategorilere göre sınıflandıran fonksiyon\n",
    "def categorize_errors(error_descriptions):\n",
    "    categories = {\n",
    "        'Database': re.compile(r'database|sql|exception|timeout|connection', re.IGNORECASE),\n",
    "        'Network': re.compile(r'network|timeout|connection|unreachable|disconnected', re.IGNORECASE),\n",
    "        'Application': re.compile(r'nullpointer|illegalargument|runtime|application|app', re.IGNORECASE),\n",
    "        'File System': re.compile(r'file|directory|not found|access denied|permission', re.IGNORECASE),\n",
    "        'Security': re.compile(r'security|unauthorized|authentication|authorization|forbidden|login', re.IGNORECASE),\n",
    "        'Performance': re.compile(r'performance|slow|timeout|delay|latency', re.IGNORECASE),\n",
    "        'Memory': re.compile(r'memory|outofmemory|heap|stack', re.IGNORECASE),\n",
    "        'Syntax': re.compile(r'syntax|parse|unexpected|unexpected token', re.IGNORECASE),\n",
    "        'Configuration': re.compile(r'configuration|config|setting|parameter|missing|invalid', re.IGNORECASE),\n",
    "        'Dependency': re.compile(r'dependency|library|module|package|not found', re.IGNORECASE),\n",
    "        'Service': re.compile(r'service|unavailable|down|restart|failed', re.IGNORECASE),\n",
    "        'Other': re.compile(r'.*')\n",
    "    }\n",
    "\n",
    "    categorized_errors = {category: 0 for category in categories}\n",
    "\n",
    "    for description in error_descriptions:\n",
    "        matched = False\n",
    "        for category, pattern in categories.items():\n",
    "            if pattern.search(description):\n",
    "                categorized_errors[category] += 1\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            categorized_errors['Other'] += 1\n",
    "\n",
    "    return categorized_errors\n",
    "\n",
    "# Streamlit arayüzü\n",
    "st.title(\"Error Log Analysis Tool\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\"Log dosyalarını yükleyin\", accept_multiple_files=True)\n",
    "\n",
    "\n",
    "\n",
    "if st.button(\"Logları İşle\"):\n",
    "    if uploaded_files:\n",
    "        # Log işleme ve analiz kodları burada\n",
    "        logs = []\n",
    "        error_logs = []\n",
    "        error_times = []\n",
    "        error_descriptions = []\n",
    "        illegal_chars = r'[^\\w\\s]'\n",
    "\n",
    "        for file in uploaded_files:\n",
    "            try:\n",
    "                lines = file.read().decode('cp1254').splitlines()\n",
    "\n",
    "                i = 0\n",
    "                while i < len(lines):\n",
    "                    if \"ERROR\" in lines[i]:\n",
    "                        try:\n",
    "                            error_start_time = re.search(r'\\d{4}-\\d{2}-\\d{2}/\\d{2}:\\d{2}:\\d{2}.\\d{3}', lines[i]).group()\n",
    "                            error_description = lines[i].split(\"ERROR\", 1)[1].strip()\n",
    "                            error_description = re.sub(r'\\[[0-9]+\\]', '', error_description)\n",
    "\n",
    "                            i += 1\n",
    "                            while i < len(lines) and not re.match(r'\\d{4}-\\d{2}-\\d{2}/\\d{2}:\\d{2}:\\d{2}.\\d{3}', lines[i]):\n",
    "                                error_description += \"\\n\" + lines[i].strip()\n",
    "                                i += 1\n",
    "\n",
    "                            error_description = re.sub(illegal_chars, ' ', error_description)\n",
    "                            error_description = error_description.replace('\\x01', hex(ord('\\x01')))\n",
    "\n",
    "                            error_logs.append({\n",
    "                                \"Tarih-saat\": pd.to_datetime(error_start_time),\n",
    "                                \"Error açıklaması\": error_description,\n",
    "                            })\n",
    "\n",
    "                            error_times.append(pd.to_datetime(error_start_time))\n",
    "                            error_descriptions.append(error_description)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error occurred while processing error logs: {str(e)}\")\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Dosya okuma hatası: {str(e)}\")\n",
    "                st.error(f\"Dosya okuma hatası: {str(e)}\")\n",
    "                st.stop()\n",
    "\n",
    "        try:\n",
    "            # Excel dosyasını oluştur ve bellek üzerinde sakla\n",
    "            df = pd.DataFrame(error_logs)\n",
    "            output = io.BytesIO()  # Bellekte dosya oluşturmak için\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                # Excel sayfalarını oluşturma ve yazma kodu\n",
    "                df.to_excel(writer, index=False, sheet_name='Error Log')\n",
    "                error_counts = df['Error açıklaması'].value_counts().reset_index()\n",
    "                error_counts.columns = ['Error', 'Count']\n",
    "                error_counts.to_excel(writer, index=False, sheet_name='Error Analiz')\n",
    "\n",
    "                # Error Counts by Time Intervals sayfası\n",
    "                error_counts_in_intervals = count_errors_in_time_intervals(error_times)\n",
    "                sorted_error_counts = dict(sorted(error_counts_in_intervals.items(), key=lambda item: pd.to_datetime(item[0])))\n",
    "                workbook = writer.book\n",
    "                worksheet = workbook.create_sheet(title='Error Counts by Time Intervals')\n",
    "                worksheet.append(['Zaman Aralığı', 'Hata Sayısı'])\n",
    "                for time, count in sorted_error_counts.items():\n",
    "                    worksheet.append([time, count])\n",
    "\n",
    "                # Hata Sayısı Çubuğu Grafiği\n",
    "                chart = BarChart()\n",
    "                chart.title = \"Hata Sayısı\"\n",
    "                chart.y_axis.title = \"Hata Sayısı\"\n",
    "                chart.x_axis.title = \"Zaman Aralığı\"\n",
    "                data = Reference(worksheet, min_col=2, min_row=2, max_row=len(sorted_error_counts) + 1)\n",
    "                cats = Reference(worksheet, min_col=1, min_row=2, max_row=len(sorted_error_counts) + 1)\n",
    "                chart.add_data(data, titles_from_data=True)\n",
    "                chart.set_categories(cats)\n",
    "                chart.height = 10\n",
    "                chart.width = 20\n",
    "                worksheet.add_chart(chart, \"D2\")\n",
    "\n",
    "                # Hata Zamanları Çizgi Grafiği\n",
    "                try:\n",
    "                    error_time_counts = Counter(error_times)\n",
    "                    sorted_error_times = dict(sorted(error_time_counts.items()))\n",
    "\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.plot(list(sorted_error_times.keys()), list(sorted_error_times.values()), marker='o')\n",
    "                    plt.title(\"Hata Sayıları (Zaman Aralıklarına Göre)\")\n",
    "                    plt.xlabel(\"Zaman\")\n",
    "                    plt.ylabel(\"Hata Sayısı\")\n",
    "                    plt.xticks(rotation=45)\n",
    "\n",
    "                    plot_path = io.BytesIO()\n",
    "                    plt.savefig(plot_path, format='png')\n",
    "                    plt.close()\n",
    "                    plot_path.seek(0)\n",
    "                    img = Image(plot_path)\n",
    "                    worksheet.add_image(img, 'A20')\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Grafik oluşturulurken bir hata oluştu: {str(e)}\")\n",
    "                    st.error(f\"Grafik oluşturulurken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "                # Specific Structure Sentences sayfası\n",
    "                specific_structure_repeated = count_specific_structure_sentences(df['Error açıklaması'])\n",
    "                if specific_structure_repeated:\n",
    "                    structure_sheet = workbook.create_sheet(title='Specific Structure Sentences')\n",
    "                    structure_sheet.append(['Cümle Yapısı', 'Sayısı'])\n",
    "                    for structure, count in specific_structure_repeated.items():\n",
    "                        structure_sheet.append([structure, count])\n",
    "\n",
    "                # Error Categories sayfası\n",
    "                categorized_errors = categorize_errors(df['Error açıklaması'])\n",
    "                if categorized_errors:\n",
    "                    category_sheet = workbook.create_sheet(title='Error Categories')\n",
    "                    category_sheet.append(['Kategori', 'Hata Sayısı'])\n",
    "                    for category, count in categorized_errors.items():\n",
    "                        category_sheet.append([category, count])\n",
    "\n",
    "            # Excel dosyasını PDF ile analiz için kapatma\n",
    "            output.seek(0)\n",
    "\n",
    "            # Excel analizini yap ve PDF oluştur\n",
    "            analyses = analyze_excel(output)\n",
    "            pdf_data = generate_analysis_pdf(analyses)\n",
    "\n",
    "            # Streamlit'te dosya indirme butonları ekleme\n",
    "            st.download_button(\"Excel Raporunu İndir\", data=output, file_name=\"error_logs.xlsx\")\n",
    "            st.download_button(\"Analiz PDF Raporunu İndir\", data=pdf_data, file_name=\"excel_analysis_report.pdf\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Loglar işlenirken hata oluştu: {str(e)}\")\n",
    "            st.error(f\"Loglar işlenirken bir hata oluştu: {str(e)}\")\n",
    "            st.stop()\n",
    "    else:\n",
    "        st.warning(\"Lütfen en az bir log dosyası yükleyin.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_analiz_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
